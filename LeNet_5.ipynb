{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnPXkY37eT2q",
        "outputId": "06502806-11bd-4f3a-e090-df65dc6e2d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 823ms/step - accuracy: 0.3999 - loss: 1.9819 - val_accuracy: 0.6756 - val_loss: 0.8773\n",
            "Epoch 2/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 797ms/step - accuracy: 0.7082 - loss: 0.7901 - val_accuracy: 0.7478 - val_loss: 0.6819\n",
            "Epoch 3/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 801ms/step - accuracy: 0.7624 - loss: 0.6401 - val_accuracy: 0.7801 - val_loss: 0.6017\n",
            "Epoch 4/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 770ms/step - accuracy: 0.7876 - loss: 0.5685 - val_accuracy: 0.7993 - val_loss: 0.5467\n",
            "Epoch 5/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 783ms/step - accuracy: 0.8077 - loss: 0.5216 - val_accuracy: 0.8032 - val_loss: 0.5306\n",
            "Epoch 6/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 762ms/step - accuracy: 0.8170 - loss: 0.4969 - val_accuracy: 0.8236 - val_loss: 0.4962\n",
            "Epoch 7/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 782ms/step - accuracy: 0.8298 - loss: 0.4690 - val_accuracy: 0.8299 - val_loss: 0.4737\n",
            "Epoch 8/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 719ms/step - accuracy: 0.8396 - loss: 0.4504 - val_accuracy: 0.8377 - val_loss: 0.4593\n",
            "Epoch 9/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 730ms/step - accuracy: 0.8488 - loss: 0.4254 - val_accuracy: 0.8417 - val_loss: 0.4429\n",
            "Epoch 10/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 769ms/step - accuracy: 0.8519 - loss: 0.4162 - val_accuracy: 0.8427 - val_loss: 0.4491\n",
            "Epoch 11/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 776ms/step - accuracy: 0.8538 - loss: 0.4081 - val_accuracy: 0.8459 - val_loss: 0.4300\n",
            "Epoch 12/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 779ms/step - accuracy: 0.8563 - loss: 0.4026 - val_accuracy: 0.8491 - val_loss: 0.4204\n",
            "Epoch 13/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 787ms/step - accuracy: 0.8602 - loss: 0.3938 - val_accuracy: 0.8532 - val_loss: 0.4148\n",
            "Epoch 14/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 763ms/step - accuracy: 0.8672 - loss: 0.3749 - val_accuracy: 0.8540 - val_loss: 0.4046\n",
            "Epoch 15/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 710ms/step - accuracy: 0.8664 - loss: 0.3783 - val_accuracy: 0.8590 - val_loss: 0.3977\n",
            "Epoch 16/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 713ms/step - accuracy: 0.8678 - loss: 0.3672 - val_accuracy: 0.8631 - val_loss: 0.3895\n",
            "Epoch 17/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 777ms/step - accuracy: 0.8711 - loss: 0.3597 - val_accuracy: 0.8644 - val_loss: 0.3802\n",
            "Epoch 18/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 773ms/step - accuracy: 0.8744 - loss: 0.3504 - val_accuracy: 0.8608 - val_loss: 0.3871\n",
            "Epoch 19/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 707ms/step - accuracy: 0.8742 - loss: 0.3520 - val_accuracy: 0.8703 - val_loss: 0.3716\n",
            "Epoch 20/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 751ms/step - accuracy: 0.8764 - loss: 0.3474 - val_accuracy: 0.8618 - val_loss: 0.3853\n",
            "Epoch 21/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 714ms/step - accuracy: 0.8756 - loss: 0.3437 - val_accuracy: 0.8701 - val_loss: 0.3662\n",
            "Epoch 22/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 762ms/step - accuracy: 0.8808 - loss: 0.3307 - val_accuracy: 0.8692 - val_loss: 0.3640\n",
            "Epoch 23/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 766ms/step - accuracy: 0.8813 - loss: 0.3307 - val_accuracy: 0.8650 - val_loss: 0.3732\n",
            "Epoch 24/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 755ms/step - accuracy: 0.8824 - loss: 0.3290 - val_accuracy: 0.8637 - val_loss: 0.3789\n",
            "Epoch 25/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 755ms/step - accuracy: 0.8829 - loss: 0.3287 - val_accuracy: 0.8746 - val_loss: 0.3530\n",
            "Epoch 26/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 695ms/step - accuracy: 0.8848 - loss: 0.3214 - val_accuracy: 0.8665 - val_loss: 0.3716\n",
            "Epoch 27/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 692ms/step - accuracy: 0.8854 - loss: 0.3204 - val_accuracy: 0.8718 - val_loss: 0.3545\n",
            "Epoch 28/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 750ms/step - accuracy: 0.8846 - loss: 0.3172 - val_accuracy: 0.8741 - val_loss: 0.3501\n",
            "Epoch 29/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 755ms/step - accuracy: 0.8891 - loss: 0.3102 - val_accuracy: 0.8765 - val_loss: 0.3484\n",
            "Epoch 30/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 720ms/step - accuracy: 0.8909 - loss: 0.3074 - val_accuracy: 0.8757 - val_loss: 0.3461\n",
            "Epoch 31/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 756ms/step - accuracy: 0.8872 - loss: 0.3088 - val_accuracy: 0.8766 - val_loss: 0.3429\n",
            "Epoch 32/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 715ms/step - accuracy: 0.8913 - loss: 0.3020 - val_accuracy: 0.8671 - val_loss: 0.3584\n",
            "Epoch 33/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 802ms/step - accuracy: 0.8882 - loss: 0.3068 - val_accuracy: 0.8790 - val_loss: 0.3373\n",
            "Epoch 34/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 763ms/step - accuracy: 0.8915 - loss: 0.2976 - val_accuracy: 0.8784 - val_loss: 0.3371\n",
            "Epoch 35/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 766ms/step - accuracy: 0.8950 - loss: 0.2921 - val_accuracy: 0.8782 - val_loss: 0.3375\n",
            "Epoch 36/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 768ms/step - accuracy: 0.8942 - loss: 0.2911 - val_accuracy: 0.8791 - val_loss: 0.3337\n",
            "Epoch 37/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 743ms/step - accuracy: 0.8965 - loss: 0.2887 - val_accuracy: 0.8812 - val_loss: 0.3297\n",
            "Epoch 38/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 718ms/step - accuracy: 0.8964 - loss: 0.2862 - val_accuracy: 0.8770 - val_loss: 0.3417\n",
            "Epoch 39/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 698ms/step - accuracy: 0.8955 - loss: 0.2863 - val_accuracy: 0.8803 - val_loss: 0.3395\n",
            "Epoch 40/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 748ms/step - accuracy: 0.8967 - loss: 0.2858 - val_accuracy: 0.8808 - val_loss: 0.3297\n",
            "Epoch 41/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 767ms/step - accuracy: 0.8976 - loss: 0.2818 - val_accuracy: 0.8791 - val_loss: 0.3343\n",
            "Epoch 42/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 760ms/step - accuracy: 0.8984 - loss: 0.2789 - val_accuracy: 0.8816 - val_loss: 0.3296\n",
            "Epoch 43/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 692ms/step - accuracy: 0.9001 - loss: 0.2747 - val_accuracy: 0.8691 - val_loss: 0.3567\n",
            "Epoch 44/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 750ms/step - accuracy: 0.8978 - loss: 0.2831 - val_accuracy: 0.8822 - val_loss: 0.3238\n",
            "Epoch 45/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 755ms/step - accuracy: 0.8972 - loss: 0.2837 - val_accuracy: 0.8838 - val_loss: 0.3202\n",
            "Epoch 46/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 759ms/step - accuracy: 0.9006 - loss: 0.2761 - val_accuracy: 0.8827 - val_loss: 0.3254\n",
            "Epoch 47/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 752ms/step - accuracy: 0.9015 - loss: 0.2699 - val_accuracy: 0.8851 - val_loss: 0.3207\n",
            "Epoch 48/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 749ms/step - accuracy: 0.9021 - loss: 0.2695 - val_accuracy: 0.8843 - val_loss: 0.3175\n",
            "Epoch 49/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 723ms/step - accuracy: 0.9037 - loss: 0.2669 - val_accuracy: 0.8813 - val_loss: 0.3233\n",
            "Epoch 50/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 703ms/step - accuracy: 0.9053 - loss: 0.2644 - val_accuracy: 0.8843 - val_loss: 0.3179\n",
            "Epoch 51/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 696ms/step - accuracy: 0.9024 - loss: 0.2646 - val_accuracy: 0.8843 - val_loss: 0.3175\n",
            "Epoch 52/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 753ms/step - accuracy: 0.9030 - loss: 0.2635 - val_accuracy: 0.8845 - val_loss: 0.3268\n",
            "Epoch 53/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 705ms/step - accuracy: 0.9035 - loss: 0.2633 - val_accuracy: 0.8852 - val_loss: 0.3200\n",
            "Epoch 54/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 790ms/step - accuracy: 0.9038 - loss: 0.2652 - val_accuracy: 0.8859 - val_loss: 0.3124\n",
            "Epoch 55/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 749ms/step - accuracy: 0.9075 - loss: 0.2589 - val_accuracy: 0.8846 - val_loss: 0.3212\n",
            "Epoch 56/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 756ms/step - accuracy: 0.9038 - loss: 0.2624 - val_accuracy: 0.8878 - val_loss: 0.3130\n",
            "Epoch 57/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 695ms/step - accuracy: 0.9073 - loss: 0.2557 - val_accuracy: 0.8866 - val_loss: 0.3186\n",
            "Epoch 58/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 738ms/step - accuracy: 0.9068 - loss: 0.2536 - val_accuracy: 0.8800 - val_loss: 0.3246\n",
            "Epoch 59/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 693ms/step - accuracy: 0.9074 - loss: 0.2543 - val_accuracy: 0.8874 - val_loss: 0.3124\n",
            "Epoch 60/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.9069 - loss: 0.2510 - val_accuracy: 0.8898 - val_loss: 0.3071\n",
            "Epoch 61/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 696ms/step - accuracy: 0.9088 - loss: 0.2513 - val_accuracy: 0.8883 - val_loss: 0.3104\n",
            "Epoch 62/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 734ms/step - accuracy: 0.9092 - loss: 0.2477 - val_accuracy: 0.8876 - val_loss: 0.3093\n",
            "Epoch 63/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 766ms/step - accuracy: 0.9088 - loss: 0.2469 - val_accuracy: 0.8888 - val_loss: 0.3113\n",
            "Epoch 64/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 755ms/step - accuracy: 0.9093 - loss: 0.2492 - val_accuracy: 0.8817 - val_loss: 0.3294\n",
            "Epoch 65/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 744ms/step - accuracy: 0.9088 - loss: 0.2483 - val_accuracy: 0.8867 - val_loss: 0.3106\n",
            "Epoch 66/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 692ms/step - accuracy: 0.9094 - loss: 0.2461 - val_accuracy: 0.8842 - val_loss: 0.3181\n",
            "Epoch 67/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 738ms/step - accuracy: 0.9107 - loss: 0.2469 - val_accuracy: 0.8877 - val_loss: 0.3122\n",
            "Epoch 68/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 696ms/step - accuracy: 0.9107 - loss: 0.2427 - val_accuracy: 0.8881 - val_loss: 0.3116\n",
            "Epoch 69/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 707ms/step - accuracy: 0.9109 - loss: 0.2410 - val_accuracy: 0.8874 - val_loss: 0.3064\n",
            "Epoch 70/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 766ms/step - accuracy: 0.9131 - loss: 0.2371 - val_accuracy: 0.8899 - val_loss: 0.3022\n",
            "Epoch 71/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 698ms/step - accuracy: 0.9137 - loss: 0.2370 - val_accuracy: 0.8892 - val_loss: 0.3050\n",
            "Epoch 72/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 700ms/step - accuracy: 0.9125 - loss: 0.2407 - val_accuracy: 0.8905 - val_loss: 0.3122\n",
            "Epoch 73/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 748ms/step - accuracy: 0.9124 - loss: 0.2379 - val_accuracy: 0.8902 - val_loss: 0.3030\n",
            "Epoch 74/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 758ms/step - accuracy: 0.9164 - loss: 0.2337 - val_accuracy: 0.8841 - val_loss: 0.3246\n",
            "Epoch 75/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 765ms/step - accuracy: 0.9119 - loss: 0.2387 - val_accuracy: 0.8894 - val_loss: 0.3049\n",
            "Epoch 76/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 717ms/step - accuracy: 0.9136 - loss: 0.2371 - val_accuracy: 0.8922 - val_loss: 0.3024\n",
            "Epoch 77/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 754ms/step - accuracy: 0.9139 - loss: 0.2366 - val_accuracy: 0.8917 - val_loss: 0.3055\n",
            "Epoch 78/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 753ms/step - accuracy: 0.9150 - loss: 0.2337 - val_accuracy: 0.8867 - val_loss: 0.3192\n",
            "Epoch 79/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 755ms/step - accuracy: 0.9163 - loss: 0.2318 - val_accuracy: 0.8904 - val_loss: 0.3087\n",
            "Epoch 80/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 699ms/step - accuracy: 0.9141 - loss: 0.2334 - val_accuracy: 0.8926 - val_loss: 0.2996\n",
            "Epoch 81/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 774ms/step - accuracy: 0.9176 - loss: 0.2262 - val_accuracy: 0.8910 - val_loss: 0.2962\n",
            "Epoch 82/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 697ms/step - accuracy: 0.9156 - loss: 0.2301 - val_accuracy: 0.8913 - val_loss: 0.2979\n",
            "Epoch 83/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 757ms/step - accuracy: 0.9175 - loss: 0.2274 - val_accuracy: 0.8895 - val_loss: 0.2990\n",
            "Epoch 84/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 759ms/step - accuracy: 0.9177 - loss: 0.2229 - val_accuracy: 0.8911 - val_loss: 0.3043\n",
            "Epoch 85/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 709ms/step - accuracy: 0.9172 - loss: 0.2236 - val_accuracy: 0.8900 - val_loss: 0.3028\n",
            "Epoch 86/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 754ms/step - accuracy: 0.9166 - loss: 0.2279 - val_accuracy: 0.8909 - val_loss: 0.3051\n",
            "Epoch 87/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 741ms/step - accuracy: 0.9187 - loss: 0.2262 - val_accuracy: 0.8894 - val_loss: 0.3020\n",
            "Epoch 88/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 780ms/step - accuracy: 0.9192 - loss: 0.2191 - val_accuracy: 0.8903 - val_loss: 0.3117\n",
            "Epoch 89/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 692ms/step - accuracy: 0.9161 - loss: 0.2282 - val_accuracy: 0.8907 - val_loss: 0.3042\n",
            "Epoch 90/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 773ms/step - accuracy: 0.9173 - loss: 0.2246 - val_accuracy: 0.8896 - val_loss: 0.3062\n",
            "Epoch 91/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 693ms/step - accuracy: 0.9210 - loss: 0.2196 - val_accuracy: 0.8882 - val_loss: 0.3143\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ce436aa06a0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import MaxPooling2D, Conv2D, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load dataset\n",
        "(X_train,y_train),(X_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Dataset to categorical\n",
        "class_count = 10\n",
        "y_train_cat = to_categorical(y_train, num_classes=class_count)\n",
        "y_test_cat = to_categorical(y_test, num_classes=class_count)\n",
        "\n",
        "# Normalise data\n",
        "X_train_norm = X_train / 255\n",
        "X_test_norm = X_test / 255\n",
        "\n",
        "# Reshape data\n",
        "X_train_norm = X_train_norm.reshape(X_train_norm.shape[0], 28, 28, 1)\n",
        "X_test_norm = X_test_norm.reshape(X_test_norm.shape[0], 28, 28, 1)\n",
        "\n",
        "\n",
        "def leNet5(num_classes):\n",
        "    model = Sequential()\n",
        "    # Layer C1\n",
        "    model.add(Conv2D(filters=6,\n",
        "                     kernel_size=(5,5),\n",
        "                     activation='relu',\n",
        "                     input_shape=(28,28,1)))\n",
        "    # Layer S2\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # Layer C3\n",
        "    model.add(Conv2D(filters=16,\n",
        "                     kernel_size=(5,5),\n",
        "                     activation='relu'))\n",
        "    # Layer S4\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # Flatten units\n",
        "    model.add(Flatten())\n",
        "    # Layer C5\n",
        "    model.add(Dense(units=120,\n",
        "                    activation='relu'))\n",
        "    # Layer F6\n",
        "    model.add(Dense(units=84,\n",
        "                    activation='relu'))\n",
        "    # Output layer\n",
        "    model.add(Dense(units=num_classes,\n",
        "                    activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Compile model\n",
        "model = leNet5(class_count)\n",
        "\n",
        "# To adjust the Learning Rate (lr), uncomment code below.\n",
        "# from keras.optimizers import Adam\n",
        "# opt = Adam(lr=0.001)\n",
        "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callback\n",
        "callback = [EarlyStopping(monitor='val_loss', patience=10)]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x=X_train_norm, y=y_train_cat, validation_data=(X_test_norm,y_test_cat), epochs=100, batch_size=2048, callbacks=callback)\n",
        "\n",
        "# Save model\n",
        "model.save(\"leNet5_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "batch_size = 1024\n",
        "y_pred_train = to_categorical(model.predict(X_train_norm,batch_size=batch_size).argmax(axis=1), num_classes=10)\n",
        "y_pred_test = to_categorical(model.predict(X_test_norm,batch_size=batch_size).argmax(axis=1), num_classes=10)\n",
        "\n",
        "print('Training Accuracy:', accuracy_score(y_pred_train, y_train_cat))\n",
        "print('Testing Accuracy:', accuracy_score(y_pred_test, y_test_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z9VqsNZe6ZI",
        "outputId": "9182753b-b794-45c7-b4cf-cfdcc91e2d86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 116ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step\n",
            "accuracy on train with NN: 0.9153666666666667\n",
            "accuracy on test with NN: 0.8882\n"
          ]
        }
      ]
    }
  ]
}